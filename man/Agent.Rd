% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agent.R
\name{Agent}
\alias{Agent}
\title{Agent}
\description{
The R6 class \code{Agent} is responsible for the state, flow of information
between and the running of one \code{Bandit} and \code{Policy} pair.
As such, multiple \code{Agent}s can be run in parallel with each separate Agent keeping
track of \code{t} and the parameters in named list \code{theta} for its assigned \code{Policy}
and \code{Bandit} pair.
}
\section{Usage}{

\preformatted{
agent <- Agent$new(policy, bandit)
}
}

\section{Arguments}{


\describe{
\item{\code{policy}}{
A \code{Policy} object, expected to take into account the current \code{d} dimensional \code{context}
feature vector \code{X}, together with a limited set of parameters denoted \code{theta} (summarizing
all past actions), to choose one of the k arms of its corresponding bandit's arms at each time step \code{t}.
}
\item{\code{bandit}}{
A \code{Bandit} object, responsible for both the generation of \code{d} dimensional \code{context}
vectors \code{X} and the \code{k} I.I.D. distributions each generating a \code{reward} for each of
its \code{k} arms at each time step \code{t}.
}
}
}

\section{Methods}{


\describe{

\item{\code{set_t(t)}}{
Setter function, sets the state of the current time step variable \code{t}.
}

\item{\code{do_step()}}{
Convenience function: completes one time step \code{t} by consecutively calling
bandit_get_context(), policy_get_action(), bandit_get_reward() and policy_set_reward().
}

\item{\code{bandit_get_context()}}{
Calls \code{bandit$get_context(t)}, which returns a named list \code{list(k = n_arms, d = n_features, X = context)}
with the current \code{d} dimensional \code{context} feature vector \code{X} together with the number of arms \code{k}.
}

\item{\code{policy_get_action()}}{
Calls \code{policy$get_action(t, X)}, whereupon \code{policy} decides on an arm to play based on the
current values of its parameters in named list \code{theta} and the current \code{context}.
Returns a named list \code{list(choice = arm_chosen_by_policy)} that holds the index of the arm
to play as suggested by \code{policy}.
}

\item{\code{bandit_get_reward()}}{
Calls \code{bandit$get_reward(t, context, action)} which returns the named list
\code{list(reward = reward_for_choice_made, optimal = optimal_reward_value)} containing the \code{reward}
for the \code{action} previously returned by \code{policy} and, optionally, the \code{optimal} reward
at the current time \code{t}.

}

\item{\code{policy_set_reward()}}{
Calls \code{policy$set_reward(t, context, action, reward)}, updating the set of parameter values in \code{theta}
based on the \code{action} taken, the \code{reward} received, and the current \code{context}.
Returns the updated list of parameters in named list \code{theta}.
}

}
}

\seealso{
Core contextual classes: \code{\link{Simulator}},
\code{\link{Agent}}, \code{\link{History}}, \code{\link{Plot}}

Bandit classes: \code{\link{Bandit}}, \code{\link{BasicBandit}},
\code{\link{LiSamplingOfflineBandit}}, \code{\link{SyntheticBandit}}

Other contextual: \code{\link{History}},
  \code{\link{Plot}}, \code{\link{Simulator}}
}
