% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit_continuum_function.R
\name{ContinuumBandit}
\alias{ContinuumBandit}
\title{Bandit: ContinuumBandit}
\description{
A function based continuum multiarmed bandit
where arms are chosen from a subset of the real line and the mean rewards
are assumed to be a continuous function of the arms.
}
\section{Usage}{

\preformatted{
   bandit <- ContinuumBandit$new(FUN)
}
}

\section{Arguments}{

\describe{
\item{FUN}{continuous function.}
}
}

\section{Methods}{


\describe{

\item{\code{new(FUN)}}{
generates and instantializes a new \code{Bandit} instance.
For arguments, see Argument section above.

}

\item{\code{get_context(t)}}{
argument:
\itemize{
\item \code{t}: integer, time step \code{t}.
}
returns a named \code{list}
optionally containing the current \code{d x k} dimensional matrix \code{context$X},
the number of arms \code{context$k} and the number of features \code{context$d}.
}

\item{\code{get_reward(t, context, action)}}{
arguments:
\itemize{
\item \code{t}: integer, time step \code{t}.
\item \code{context}: list, containing the current \code{context$X} (d x k context matrix),
\code{context$k} (number of arms) and \code{context$d} (number of context feaures)
(as set by \code{bandit}).
\item \code{action}:  list, containing \code{action$choice} (as set by \code{policy}).
}
returns a named \code{list} containing \code{reward$reward}
}
}
}

\examples{
\donttest{

horizon            <- 1500
simulations        <- 100

continuous_arms  <- function(x) {
  -0.1*(x - 5) ^ 2 + 3.5  + rnorm(length(x),0,0.4)
}

int_time    <- 100
amplitude   <- 0.2
learn_rate  <- 0.3
omega       <- 2*pi/int_time
x0_start    <- 2.0

policy             <- LifPolicy$new(int_time, amplitude, learn_rate, omega, x0_start)

bandit             <- ContinuumBandit$new(FUN = continuous_arms)

agent              <- Agent$new(policy,bandit)

history            <- Simulator$new(     agents = agent,
                                         horizon = horizon,
                                         simulations = simulations,
                                         save_theta = TRUE             )$run()

plot(history, type = "average", regret = FALSE)
}

}
\seealso{
Core contextual classes: \code{\link{Bandit}}, \code{\link{Policy}}, \code{\link{Simulator}},
\code{\link{Agent}}, \code{\link{History}}, \code{\link{Plot}}

Other contextual subclasses: \code{\link{ContextualEpochGreedyPolicy}},
  \code{\link{ContextualThompsonSamplingPolicy}},
  \code{\link{EpsilonFirstPolicy}},
  \code{\link{EpsilonGreedyPolicy}},
  \code{\link{Exp3Policy}},
  \code{\link{GittinsBrezziLaiPolicy}},
  \code{\link{LifPolicy}},
  \code{\link{LinUCBDisjointOptimizedPolicy}},
  \code{\link{LinUCBDisjointPolicy}},
  \code{\link{LinUCBGeneralPolicy}},
  \code{\link{LinUCBHybridOptimizedPolicy}},
  \code{\link{LinUCBHybridPolicy}},
  \code{\link{OfflinePolicyEvaluatorBandit}},
  \code{\link{OraclePolicy}}, \code{\link{RandomPolicy}},
  \code{\link{SoftmaxPolicy}},
  \code{\link{ThompsonSamplingPolicy}},
  \code{\link{UCB1Policy}}
}
