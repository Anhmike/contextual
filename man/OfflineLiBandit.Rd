% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit_li.R
\name{OfflineLiBandit}
\alias{OfflineLiBandit}
\title{Bandit: Li Offline Evaluation}
\description{
\code{OfflineLiBandit} uses data from a randomly assigned policy for offline evaluation.
The key assumption of the method is that the individual events are i.i.d., and
that the logging policy chose each arm at each time step uniformly at random.
Take care: if A is a stationary policy that does not change over trials,
data may be used more efficiently via propensity
scoring (Langford et al., 2008; Strehl et al., 2011) and related
techniques like doubly robust estimation (Dudik et al., 2011).
}
\section{Usage}{

\preformatted{
bandit <- OfflineLiBandit(data_file, k, d)
}
}

\examples{

horizon            <- 100L

}
\references{
Li, L., Chu, W., Langford, J., & Wang, X. (2011, February). Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 297-306). ACM.
}
\seealso{
Core contextual classes: \code{\link{Contextual}}, \code{\link{Simulator}},
\code{\link{Agent}}, \code{\link{History}}, \code{\link{Plot}}, \code{\link{AbstractPolicy}}
}
