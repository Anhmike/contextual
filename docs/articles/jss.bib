% Encoding: UTF-8

@Manual{contextual,
  title        = {Contextual: Simulating Contextual Multi-armed Bandit Problems in R},
  author       = {van Emden, R. and Kaptein, M. and Postma, E.},
  organization = {Jheronimus Academy of Data Science},
  month        = jun,
  year         = {2018},
  journal      = {In preparation for submission to The Journal of Statistical Software},
}

@Article{Wilson2014,
  author    = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
  title     = {Humans Use Directed and Random Exploration to Solve the Explore--exploit Dilemma.},
  journal   = {Journal of Experimental Psychology: General},
  year      = {2014},
  volume    = {143},
  number    = {6},
  pages     = {2074},
  doi       = {10.1037/a0038199},
  publisher = {American Psychological Association},
}

@Article{Whittle1979,
  author  = {Whittle, P.},
  title   = {Discussion on "bandit Processes and Dynamic Allocation Indices"},
  journal = {J. Roy. Statist. Soc. Ser. B},
  year    = {1979},
  volume  = {41},
  pages   = {165},
}

@Article{Bubeck2012,
  author    = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and others},
  title     = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  year      = {2012},
  volume    = {5},
  number    = {1},
  pages     = {1--122},
  doi       = {10.1561/2200000024},
  publisher = {Now Publishers, Inc.},
}

@InProceedings{Langford2008,
  author    = {Langford, John and Zhang, Tong},
  title     = {The Epoch-greedy Algorithm for Multi-armed Bandits with Side Information},
  booktitle = {Advances in neural information processing systems},
  year      = {2008},
  pages     = {817--824},
}

@InCollection{Tewari2017,
  author    = {Tewari, Ambuj and Murphy, Susan A.},
  title     = {From Ads to Interventions: Contextual Bandits in Mobile Health},
  booktitle = {Mobile Health},
  publisher = {Springer},
  year      = {2017},
  pages     = {495--517},
  doi       = {10.1007/978-3-319-51394-2_25},
}

@Article{Wang2005a,
  author    = {Wang, Chih-Chun and Kulkarni, Sanjeev R. and Poor, H. Vincent},
  title     = {Arbitrary Side Observations in Bandit Problems},
  journal   = {Advances in Applied Mathematics},
  year      = {2005},
  volume    = {34},
  number    = {4},
  pages     = {903--938},
  doi       = {10.1016/j.aam.2004.10.004},
  publisher = {Elsevier},
}

@InProceedings{Lu2010,
  author    = {Lu, Tyler and P{\'a}l, D{\'a}vid and P{\'a}l, Martin},
  title     = {Contextual Multi-armed Bandits},
  booktitle = {Proceedings of the Thirteenth international conference on Artificial Intelligence and Statistics},
  year      = {2010},
  pages     = {485--492},
}

@Article{Kaelbling1996,
  author  = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  title   = {Reinforcement Learning: A Survey},
  journal = {Journal of artificial intelligence research},
  year    = {1996},
  volume  = {4},
  pages   = {237--285},
  doi     = {10.1613/jair.301},
}

@Article{Abe2003,
  author    = {Abe, Naoki and Biermann, Alan W. and Long, Philip M.},
  title     = {Reinforcement Learning with Immediate Rewards and Linear Hypotheses},
  journal   = {Algorithmica},
  year      = {2003},
  volume    = {37},
  number    = {4},
  pages     = {263--293},
  doi       = {10.1007/s00453-003-1038-1},
  publisher = {Springer},
}

@InProceedings{Strehl2006,
  author       = {Strehl, Alexander L. and Mesterharm, Chris and Littman, Michael L. and Hirsh, Haym},
  title        = {Experience-efficient Learning in Associative Bandit Problems},
  booktitle    = {Proceedings of the 23\textsuperscript{rd} international conference on Machine learning},
  year         = {2006},
  pages        = {889--896},
  organization = {ACM},
  doi          = {10.1145/1143844.1143956},
}

@Article{Sarkar1991,
  author    = {Sarkar, Jyotirmoy},
  title     = {One-armed Bandit Problems with Covariates},
  journal   = {The Annals of Statistics},
  year      = {1991},
  pages     = {1978--2002},
  doi       = {10.1214/aos/1176348382},
  publisher = {JSTOR},
}

@Article{Lai1985,
  author    = {Lai, Tze Leung and Robbins, Herbert},
  title     = {Asymptotically Efficient Adaptive Allocation Rules},
  journal   = {Advances in applied mathematics},
  year      = {1985},
  volume    = {6},
  number    = {1},
  pages     = {4--22},
  doi       = {10.1016/0196-8858(85)90002-8},
  publisher = {Academic Press},
}

@Article{Auer2002,
  author    = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  title     = {Finite-time Analysis of the Multiarmed Bandit Problem},
  journal   = {Machine learning},
  year      = {2002},
  volume    = {47},
  number    = {2-3},
  pages     = {235--256},
  publisher = {Springer},
}

@InProceedings{Li2010,
  author       = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
  title        = {A Contextual-bandit Approach to Personalized News Article Recommendation},
  booktitle    = {Proceedings of the 19\textsuperscript{th} international conference on World wide web},
  year         = {2010},
  pages        = {661--670},
  organization = {ACM},
  doi          = {10.1145/1772690.1772758},
}

@InProceedings{Tang2013,
  author       = {Tang, Liang and Rosales, Romer and Singh, Ajit and Agarwal, Deepak},
  title        = {Automatic Ad Format Selection Via Contextual Bandits},
  booktitle    = {Proceedings of the 22\textsuperscript{nd} ACM international conference on Conference on information \& knowledge management},
  year         = {2013},
  pages        = {1587--1594},
  organization = {ACM},
}

@Misc{Langford2007,
  author   = {Langford, John and Li, Lihong and Strehl, Alex},
  title    = {Vowpal Wabbit},
  year     = {2007},
  keywords = {cj-bib},
}

@Article{kruijswijk2018streamingbandit,
  author    = {Kruijswijk, Jules and Parvinen, Petri and van Emden, Robin and Kaptein, Maurits C.},
  title     = {Streamingbandit; Experimenting with Bandit Policies},
  journal   = {Journal of Statistical Software},
  year      = {2018},
  publisher = {Foundation for Open Access Statistics},
}

@Electronic{striatum,
  author     = {NTUCSIE-CLLab},
  year       = {2018},
  title      = {Striatum: {Contextual} Bandit in Python},
  url        = {https://github.com/ntucllab/striatum},
  copyright  = {BSD-2-Clause},
  shorttitle = {striatum},
  urldate    = {2018-07-31TZ},
}

@Misc{SMPyBandits,
  author       = {Lilian Besson},
  title        = {Smpybandits: An Open-source Research Framework for Single and Multi-players Multi-arms Bandits (mab) Algorithms in Python},
  howpublished = {Online at: \url{github.com/SMPyBandits/SMPyBandits}},
  year         = {2018},
  note         = {Code at https://github.com/SMPyBandits/SMPyBandits/, documentation at https://smpybandits.github.io/},
  url          = {https://github.com/SMPyBandits/SMPyBandits/},
}

@Electronic{RCore,
  author       = {{R Core Team}},
  year         = {2018},
  title        = {R: A Language and Environment for Statistical Computing},
  language     = {English},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  url          = {https://www.R-project.org},
}

@InProceedings{Li2011,
  author    = {Li, Lihong and Chu, Wei and Langford, John and Wang, Xuanhui},
  title     = {Unbiased {Offline} {Evaluation} of {Contextual}-bandit-based {News} {Article} {Recommendation} {Algorithms}},
  booktitle = {Proceedings of the {Fourth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
  year      = {2011},
  series    = {{WSDM} '11},
  pages     = {297--306},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. Offline evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their partial-label nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a replay methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.},
  doi       = {10.1145/1935826.1935878},
  isbn      = {9781450304931},
  keywords  = {benchmark dataset, contextual bandit, multi-armed bandit, offline evaluation, recommendation},
  pdf       = {pdf\:PDF:pdf\:ACM Full Text PDF/\:http//\://dl.acm.org/ft_gateway.cfm?id=1935878&type=pdf/\:application/pdf\:PDF:PDF},
  urldate   = {2018-08-01TZ},
}

@InProceedings{Mousavi2016,
  author       = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  title        = {Deep Reinforcement Learning: An Overview},
  booktitle    = {Proceedings of SAI Intelligent Systems Conference},
  year         = {2016},
  pages        = {426--440},
  organization = {Springer},
  doi          = {10.1007/978-3-319-56991-8_32},
}

@InProceedings{Seldin2011,
  author    = {Seldin, Yevgeny and Auer, Peter and Shawe-Taylor, John S. and Ortner, Ronald and Laviolette, Fran{\c{c}}ois},
  title     = {Pac-bayesian Analysis of Contextual Bandits},
  booktitle = {Advances in neural information processing systems},
  year      = {2011},
  pages     = {1683--1691},
}

@Article{Li2017,
  author  = {Li, Yuxi},
  title   = {Deep Reinforcement Learning: An Overview},
  journal = {arXiv preprint arXiv:1701.07274},
  year    = {2017},
}

@InProceedings{Agarwal2014,
  author    = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  title     = {Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits},
  booktitle = {International Conference on Machine Learning},
  year      = {2014},
  pages     = {1638--1646},
}

@Article{Sutton1998e,
  author   = {R. S. Sutton and A. G. Barto},
  title    = {Reinforcement Learning: An Introduction},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {1998},
  volume   = {9},
  number   = {5},
  pages    = {1054},
  month    = sep,
  issn     = {1045-9227},
  doi      = {10.1109/TNN.1998.712192},
  keywords = {Books, Neural networks, Dynamic programming, Machine learning, Learning systems, Artificial intelligence, Artificial neural networks, Bibliographies, Neurofeedback, Function approximation},
}

@Article{Agrawal2011,
  author        = {Shipra Agrawal and Navin Goyal},
  title         = {Analysis of Thompson Sampling for the Multi-armed Bandit Problem},
  journal       = {arXiv},
  year          = {2011},
  month         = nov,
  abstract      = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time $T$ is $O(\frac{\ln T}{\Delta} + \frac{1}{\Delta^3})$. And, for the $N$-armed bandit problem, the expected regret in time $T$ is $O([(\sum_{i=2}^N \frac{1}{\Delta_i^2})^2] \ln T)$. Our bounds are optimal but for the dependence on $\Delta_i$ and the constant factors in big-Oh.},
  archiveprefix = {arXiv},
  eprint        = {http://arxiv.org/abs/1111.1797v3},
  keywords      = {cs.LG, cs.DS, 68W40, 68Q25, F.2.0},
  pdf           = {\:PDF\:PDF:1111.1797v3/\:PDF\:/\:http//\://arxiv.org/pdf/1111.1797v3/\:PDF\:PDF:PDF},
  primaryclass  = {cs.LG},
}

@Article{Eckles2014,
  author        = {Dean Eckles and Maurits Kaptein},
  title         = {Thompson Sampling with the Online Bootstrap},
  journal       = {arXiv},
  year          = {2014},
  month         = oct,
  abstract      = {Thompson sampling provides a solution to bandit problems in which new observations are allocated to arms with the posterior probability that an arm is optimal. While sometimes easy to implement and asymptotically optimal, Thompson sampling can be computationally demanding in large scale bandit problems, and its performance is dependent on the model fit to the observed data. We introduce bootstrap Thompson sampling (BTS), a heuristic method for solving bandit problems which modifies Thompson sampling by replacing the posterior distribution used in Thompson sampling by a bootstrap distribution. We first explain BTS and show that the performance of BTS is competitive to Thompson sampling in the well-studied Bernoulli bandit case. Subsequently, we detail why BTS using the online bootstrap is more scalable than regular Thompson sampling, and we show through simulation that BTS is more robust to a misspecified error distribution. BTS is an appealing modification of Thompson sampling, especially when samples from the posterior are otherwise not available or are costly.},
  archiveprefix = {arXiv},
  eprint        = {http://arxiv.org/abs/1410.4009v1},
  keywords      = {cs.LG, stat.CO, stat.ML, 68W27, 62L05, G.3; I.2.6},
  pdf           = {\:PDF\:PDF:1410.4009v1/\:PDF\:/\:http//\://arxiv.org/pdf/1410.4009v1/\:PDF\:PDF:PDF},
  primaryclass  = {cs.LG},
}

@Article{Agrawal2012a,
  author        = {Shipra Agrawal and Navin Goyal},
  title         = {Thompson Sampling for Contextual Bandits with Linear Payoffs},
  journal       = {arXiv},
  year          = {2012},
  month         = sep,
  abstract      = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of $\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for this problem.},
  archiveprefix = {arXiv},
  eprint        = {http://arxiv.org/abs/1209.3352v4},
  keywords      = {cs.LG, cs.DS, stat.ML, 68W40, 68Q25, F.2.0},
  pdf           = {\:PDF\:PDF:1209.3352v4/\:PDF\:/\:http//\://arxiv.org/pdf/1209.3352v4/\:PDF\:PDF:PDF},
  primaryclass  = {cs.LG},
}

@Article{Auer2003,
  author    = {Auer, Peter},
  title     = {Using {Confidence} {Bounds} for {Exploitation}-exploration {Trade}-offs},
  journal   = {J. Mach. Learn. Res.},
  year      = {2003},
  volume    = {3},
  pages     = {397--422},
  month     = mar,
  issn      = {1532-4435},
  abstract  = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
  keywords  = {bandit problem, exploitation-exploration, linear value function, online Learning, reinforcement learning},
  pdf       = {pdf\:PDF:pdf\:ACM Full Text PDF/\:http//\://dl.acm.org/ft_gateway.cfm?id=944941&type=pdf/\:application/pdf\:PDF:PDF},
  publisher = {JMLR.org},
  url       = {http://dl.acm.org/citation.cfm?id=944919.944941},
  urldate   = {2018-08-03TZ},
}

@PhdThesis{Li2016,
  author   = {Li, Shuai},
  title    = {The Art of Clustering Bandits.},
  school   = {Universit{\`{a}} degli Studi dell'Insubria},
  year     = {2016},
  abstract = {Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithms could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. The purpose of this thesis is to introduce novel and principled algorithmic approaches to the solution of such networked bandit problems. Starting from a global (Laplacian-based) strategy which allocates a bandit algorithm to each network node (user), and allows it to share signals (contexts and payoffs) with the neghboring nodes, our goal is to derive and experimentally test more scalable approaches based on different ways of clustering the graph nodes. More importantly, we shall investigate the case when the graph structure is not given ahead of time, and has to be inferred based on past user behavior. A general difficulty arising in such practical scenarios is that data sequences are typically nonstationary, implying that traditional statistical inference methods should be used cautiously, possibly replacing them with by more robust nonstochastic (e.g., game-theoretic) inference methods.
In this thesis, we will firstly introduce the centralized clustering bandits. Then, we propose the corresponding solution in decentralized scenario. After that, we explain the generic collaborative clustering bandits. Finally, we extend and showcase the state-of-the-art clustering bandits that we developed in the quantification problem.},
  language = {eng},
  pdf      = {html\:PDF:html\:Full Text PDF/\:http//\://insubriaspace.cineca.it/bitstream/10277/729/1/PhD_Thesis_Lishuai_completa.pdf/\:application/pdf/\;Snapshot/\:http//\://insubriaspace.cineca.it/handle/10277/729/\:text/html\:PDF:PDF},
  url      = {http://insubriaspace.cineca.it/handle/10277/729},
  urldate  = {CURRENT\_TIMESTAMP},
}

@InProceedings{Chu2009,
  author     = {Chu, Wei and Park, Seung-Taek and Beaupre, Todd and Motgi, Nitin and Phadke, Amit and Chakraborty, Seinjuti and Zachariah, Joe},
  title      = {A Case Study of Behavior-driven Conjoint Analysis on {Yahoo}!: Front Page Today Module},
  booktitle  = {Proceedings of the 15\textsuperscript{th} {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
  year       = {2009},
  pages      = {1097--1104},
  publisher  = {ACM},
  doi        = {10.1145/1557019.1557138},
  pdf        = {\:PDF\:PDF:pdf/\:PDF\:pdf/\:Snapshot//\:https///\://dl.acm.org/citation.cfm?id=1557138//\://\;Fulltext//\:http///\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3657&rep=rep1&type=pdf//\:application/pdf/\:PDF\:PDF:PDF},
  shorttitle = {A case study of behavior-driven conjoint analysis on {Yahoo}!},
}

@Article{IdreosGNMMK12,
  author   = {Stratos Idreos and Fabian Groffen and Niels Nes and Stefan Manegold and K. Sjoerd Mullender and Martin L. Kersten},
  title    = {Monetdb: Two Decades of Research in Column-oriented Database Architectures},
  journal  = {IEEE Data Engineering Bulletin},
  year     = {2012},
  volume   = {35},
  number   = {1},
  pages    = {40--45},
  abstract = {MonetDB is a state-of-the-art open-source column-store database management system targeting applications in need for analytics over large collections of data. MonetDB is actively used nowadays in health care, in telecommunications as well as in scientific databases and in data management research, accumulating on average more than 10,000 downloads on a monthly basis. This paper gives a brief overview of the MonetDB technology as it developed over the past two decades and the main research highlights which drive the current MonetDB design and form the basis for its future evolution.},
}

@InProceedings{Vermorel2005,
  author    = {Vermorel, Joannes and Mohri, Mehryar},
  title     = {Multi-armed Bandit Algorithms and Empirical Evaluation},
  booktitle = {European conference on machine learning},
  year      = {2005},
  pages     = {437--448},
  publisher = {Springer},
  doi       = {10.1007/11564096_42},
  pdf       = {pdf\:PDF:pdf\:Snapshot/\:https//\://link.springer.com/chapter/10.1007/11564096_42/\:/\;Fulltext/\:http//\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.4518&rep=rep1&type=pdf/\:application/pdf\:PDF:PDF},
}

@Article{Brezzi2002,
  author    = {Brezzi, Monica and Lai, Tze Leung},
  title     = {Optimal Learning and Experimentation in Bandit Problems},
  journal   = {Journal of Economic Dynamics and Control},
  year      = {2002},
  volume    = {27},
  number    = {1},
  pages     = {87--108},
  doi       = {10.1016/s0165-1889(01)00028-8},
  pdf       = {pdf\:PDF:pdf\:Snapshot\\\:https\\\\\\\://www.sciencedirect.com/science/article/pii/S0165188901000288\\\:\\\;Fulltext\\\:https\\\\\\\://pdfs.semanticscholar.org/42e2/708e1d6c73ecef5ddc4fd7935a4f055de774.pdf\\\:application/pdf\:PDF:PDF},
  publisher = {Elsevier},
}

@Article{Kaptein2016a,
  author     = {Kaptein, Maurits C. and Van Emden, Robin and Iannuzzi, Davide},
  title      = {Tracking the Decoy: Maximizing the Decoy Effect through Sequential Experimentation},
  journal    = {Palgrave Communications},
  year       = {2016},
  volume     = {2},
  pages      = {16082},
  doi        = {10.1057/palcomms.2016.82},
  pdf        = {html\:PDF:html\:Snapshot\\\:https\\\\\\\://www.nature.com/articles/palcomms201682\\\:\\\;Fulltext\\\:https\\\\\\\://www.nature.com/articles/palcomms201682\\\:text/html\:PDF:PDF},
  publisher  = {Nature Publishing Group},
  shorttitle = {Tracking the decoy},
}

@Article{Tippmann2015,
  author     = {Tippmann, Sylvia},
  title      = {Programming Tools: {Adventures} with {R}},
  journal    = {Nature News},
  year       = {2015},
  volume     = {517},
  number     = {7532},
  pages      = {109},
  pdf        = {pdf\:PDF:pdf\:Snapshot\\\:https\\\\\\\://www.nature.com/articles/517109a\\\:\\\;Fulltext\\\:http\\\\\\\://www.startfactor.pt/uploads/3/8/5/0/38505347/rinnature.pdf\\\:application/pdf\:PDF:PDF},
  shorttitle = {Programming tools},
}

@Misc{2012,
  author   = {Muenchen, Robert A.},
  title    = {The {Popularity} of {Data} {Science} {Software}},
  month    = apr,
  year     = {2012},
  abstract = {by Robert A. Muenchen Abstract This article, formerly known as The Popularity of Data Analysis Software, presents various ways of measuring the popularity or market share of software for advanced a{\ldots}},
  journal  = {r4stats.com},
  language = {en},
  pdf      = {html\:PDF:html\:Snapshot/\:http//\://r4stats.com/articles/popularity//\:text/html\:PDF:PDF},
  url      = {http://r4stats.com/articles/popularity/},
  urldate  = {CURRENT\_TIMESTAMP},
}

@Manual{R6,
  title  = {R6: Classes with Reference Semantics},
  author = {Winston Chang},
  year   = {2017},
  note   = {R package version 2.2.2},
  url    = {https://CRAN.R-project.org/package=R6},
}

@Article{Mesirov2010,
  author    = {Mesirov, Jill P.},
  title     = {Accessible Reproducible Research},
  journal   = {Science},
  year      = {2010},
  volume    = {327},
  number    = {5964},
  pages     = {415--416},
  doi       = {10.1126/science.1179653},
  pdf       = {html\:PDF:html\:Snapshot\\\:http\\\\\\\://science.sciencemag.org/content/327/5964/415.short\\\:\\\;Fulltext\\\:https\\\\\\\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3878063/\\\:text/html\:PDF:PDF},
  publisher = {American Association for the Advancement of Science},
}

@Book{Gandrud2016,
  title     = {Reproducible Research with {R} and {R} Studio},
  publisher = {Chapman and Hall/CRC},
  year      = {2016},
  author    = {Gandrud, Christopher},
  pdf       = {\:\:PDF:9781498715386\\\:\:Snapshot\\\:https\\\\\\\://www.taylorfrancis.com/books/9781498715386\\\:\:PDF:PDF},
}

@InCollection{Buckheit1995,
  author    = {Buckheit, Jonathan B. and Donoho, David L.},
  title     = {Wavelab and Reproducible Research},
  booktitle = {Wavelets and statistics},
  publisher = {Springer},
  year      = {1995},
  pages     = {55--81},
  doi       = {10.1007/978-1-4612-2544-7_5},
  pdf       = {pdf\:PDF:pdf\:Snapshot/\:https//\://link.springer.com/10.1007/978-1-4612-2544-7_5/\:/\;Fulltext/\:https//\://statistics.stanford.edu/sites/default/files/EFS%20NSF%20474.pdf/\:application/pdf\:PDF:PDF},
}

@Article{Stodden2013,
  author     = {Stodden, Victoria and Guo, Peixuan and Ma, Zhaokun},
  title      = {Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals},
  journal    = {PloS one},
  year       = {2013},
  volume     = {8},
  number     = {6},
  pages      = {e67111},
  doi        = {10.1371/journal.pone.0067111},
  pdf        = {html\:PDF:html\:Snapshot/\:http//\://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067111/\:/\;Fulltext/\:http//\://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067111/\:text/html\:PDF:PDF},
  publisher  = {Public Library of Science},
  shorttitle = {Toward reproducible computational research},
}

@Book{wickham2014advanced,
  title     = {Advanced R},
  publisher = {Chapman and Hall/CRC},
  year      = {2014},
  author    = {Wickham, Hadley},
  doi       = {10.1201/b17487},
}

@Article{Dudik2011,
  author  = {Dud{\'{i}}k, Miroslav and Langford, John and Li, Lihong},
  title   = {Doubly Robust Policy Evaluation and Learning},
  journal = {arXiv preprint arXiv:1103.4601},
  year    = {2011},
  pdf     = {pdf\:PDF:pdf\:Snapshot/\:https//\://arxiv.org/abs/1103.4601/\:/\;Fulltext/\:https//\://arxiv.org/pdf/1103.4601/\:application/pdf\:PDF:PDF},
}

@Book{Zheng2016,
  title     = {Sequential {Learning} and {Decision}-{Making} in {Wireless} {Resource} {Management}},
  publisher = {Springer},
  year      = {2016},
  author    = {Zheng, Rong and Hua, Cunqing},
  doi       = {10.1007/978-3-319-50502-2},
  pdf       = {\:\:PDF:978-3-319-50502-2.pdf/\:\:Snapshot/\:https//\://link.springer.com/content/pdf/10.1007/978-3-319-50502-2.pdf/\:\:PDF:PDF},
}

@Article{Agrawal1995,
  author    = {Agrawal, Rajeev},
  title     = {The Continuum-armed Bandit Problem},
  journal   = {SIAM journal on control and optimization},
  year      = {1995},
  volume    = {33},
  number    = {6},
  pages     = {1926--1951},
  doi       = {10.1137/s0363012992237273},
  pdf       = {\:\:PDF:S0363012992237273/\:\:Snapshot/\:https//\://epubs.siam.org/doi/abs/10.1137/S0363012992237273/\:\:PDF:PDF},
  publisher = {SIAM},
}

@InCollection{Zheng2016a,
  author    = {Zheng, Rong and Hua, Cunqing},
  title     = {Stochastic {Multi}-armed {Bandit}},
  booktitle = {Sequential {Learning} and {Decision}-{Making} in {Wireless} {Resource} {Management}},
  publisher = {Springer},
  year      = {2016},
  pages     = {9--25},
  doi       = {10.1007/978-3-319-50502-2_2},
  pdf       = {\:\:PDF:978-3-319-50502-2_2/\:\:Snapshot/\:https//\://link.springer.com/chapter/10.1007/978-3-319-50502-2_2/\:\:PDF:PDF},
}

@Article{Kuleshov2014,
  author  = {Kuleshov, Volodymyr and Precup, Doina},
  title   = {Algorithms for Multi-armed Bandit Problems},
  journal = {arXiv preprint arXiv:1402.6028},
  year    = {2014},
  pdf     = {pdf:Snapshot\:https/\://arxiv.org/abs/1402.6028\:\;Fulltext\:https/\://arxiv.org/pdf/1402.6028\:application/pdf:PDF},
}

@InProceedings{Langford2008a,
  author    = {Langford, John and Strehl, Alexander and Wortman, Jennifer},
  title     = {Exploration Scavenging},
  booktitle = {Proceedings of the 25th international conference on {Machine} learning},
  year      = {2008},
  pages     = {528--535},
  publisher = {ACM},
  pdf       = {Snapshot:https\://dl.acm.org/citation.cfm?id=1390223:;Fulltext:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.6809&rep=rep1&type=pdf:application/pdf},
}

@InProceedings{Li2012,
  author    = {Li, Lihong and Chu, Wei and Langford, John and Moon, Taesup and Wang, Xuanhui},
  title     = {An Unbiased Offline Evaluation of Contextual Bandit Algorithms with Generalized Linear Models},
  booktitle = {Proceedings of the {Workshop} on {On}-line {Trading} of {Exploration} and {Exploitation} 2},
  year      = {2012},
  pages     = {19--36},
  pdf       = {Snapshot:http\://www.jmlr.org/proceedings/papers/v26/li12a/li12a.pdf:;Fulltext:http\://www.jmlr.org/proceedings/papers/v26/li12a/li12a.pdf:application/pdf},
}

@InProceedings{Strehl2006a,
  author    = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
  title     = {{PAC} Model-free Reinforcement Learning},
  booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
  year      = {2006},
  pages     = {881--888},
  publisher = {ACM},
  pdf       = {Snapshot:https\://dl.acm.org/citation.cfm?id=1143955:;Fulltext:https\://www.researchgate.net/profile/Michael_Littman2/publication/200744532_PAC_model-free_reinforcement_learning/links/54b66cb60cf24eb34f6d19d8/PAC-model-free-reinforcement-learning.pdf:application/pdf},
}

@Article{Bastani2015,
  author  = {Bastani, Hamsa and Bayati, Mohsen},
  title   = {Online Decision-making with High-dimensional Covariates},
  journal = {SSRN},
  year    = {2015},
  pdf     = {Snapshot:https\://papers.ssrn.com/sol3/papers.cfm?abstract_id=2661896:;Fulltext:http\://web.stanford.edu/~bayati/papers/lassoBandit.pdf:application/pdf},
}

@InProceedings{Mandel2016,
  author    = {Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovic, Zoran},
  title     = {Offline {Evaluation} of {Online} {Reinforcement} {Learning} {Algorithms}.},
  booktitle = {{AAAI}},
  year      = {2016},
  pages     = {1926--1933},
  pdf       = {Snapshot:http\://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12475/11824:;Fulltext:http\://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12475/11824:application/pdf},
}

@InProceedings{Strehl2010,
  author    = {Strehl, Alex and Langford, John and Li, Lihong and Kakade, Sham M.},
  title     = {Learning from Logged Implicit Exploration Data},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  year      = {2010},
  pages     = {2217--2225},
  pdf       = {Snapshot:http\://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data:;Fulltext:http\://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data.pdf:application/pdf},
}

@Book{Wirfs-Brock1990,
  title     = {Designing Object-oriented Software},
  publisher = {CUMINCAD},
  year      = {1990},
  author    = {Wirfs-Brock, Rebecca and Wilkerson, Brian and Wiener, Lauren},
  pdf       = {Snapshot:http\://papers.cumincad.org/cgi-bin/works/Show?81d1:},
}

@Book{Rumbaugh2004,
  title     = {Unified Modeling Language Reference Manual},
  publisher = {Pearson Higher Education},
  year      = {2004},
  author    = {Rumbaugh, James and Jacobson, Ivar and Booch, Grady},
  pdf       = {Snapshot:https\://dl.acm.org/citation.cfm?id=993859:},
}

@Article{Kirkpatrick1983,
  author    = {Kirkpatrick, Scott and Gelatt, C. Daniel and Vecchi, Mario P.},
  title     = {Optimization by Simulated Annealing},
  journal   = {science},
  year      = {1983},
  volume    = {220},
  number    = {4598},
  pages     = {671--680},
  pdf       = {Snapshot:http\://science.sciencemag.org/content/220/4598/671.short:;Fulltext:http\://sci2s.ugr.es/sites/default/files/files/Teaching/GraduatesCourses/Metaheuristicas/Bibliography/1983-Science-Kirkpatrick-sim_anneal.pdf:application/pdf},
  publisher = {American Association for the Advancement of Science},
}

@InProceedings{Cesa-Bianchi1998,
  author    = {Cesa-Bianchi, Nicolo and Fischer, Paul},
  title     = {Finite-{Time} {Regret} {Bounds} for the {Multiarmed} {Bandit} {Problem}.},
  booktitle = {{ICML}},
  year      = {1998},
  pages     = {100--108},
  publisher = {Citeseer},
  pdf       = {Snapshot:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.4710&rep=rep1&type=pdf:;Fulltext:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.4710&rep=rep1&type=pdf:application/pdf},
}

@Article{Presman1991,
  author    = {Presman, Ernst L.},
  title     = {Poisson Version of the Two-armed Bandit Problem with Discounting},
  journal   = {Theory of Probability \& Its Applications},
  year      = {1991},
  volume    = {35},
  number    = {2},
  pages     = {307--317},
  pdf       = {Snapshot:https\://epubs.siam.org/doi/pdf/10.1137/1135038:},
  publisher = {SIAM},
}

@Article{Kaptein2018,
  author    = {Kaptein, Maurits and McFarland, Richard and Parvinen, Petri},
  title     = {Automated Adaptive Selling},
  journal   = {European Journal of Marketing},
  year      = {2018},
  volume    = {52},
  number    = {5/6},
  pages     = {1037--1059},
  pdf       = {Snapshot:https\://www.emeraldinsight.com/doi/abs/10.1108/EJM-08-2016-0485:},
  publisher = {Emerald Publishing Limited},
}

@InProceedings{Shen2015,
  author    = {Shen, Weiwei and Wang, Jun and Jiang, Yu-Gang and Zha, Hongyuan},
  title     = {Portfolio {Choices} with {Orthogonal} {Bandit} {Learning}.},
  booktitle = {{IJCAI}},
  year      = {2015},
  volume    = {15},
  pages     = {974--980},
  pdf       = {Snapshot:http\://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/10972/10798:;Fulltext:http\://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/10972/10798:application/pdf},
}

@InProceedings{Rabbi2015,
  author     = {Rabbi, Mashfiqui and Aung, Min Hane and Zhang, Mi and Choudhury, Tanzeem},
  title      = {{MyBehavior}: Automatic Personalized Health Feedback from User Behaviors and Preferences Using Smartphones},
  booktitle  = {Proceedings of the 2015 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
  year       = {2015},
  pages      = {707--718},
  publisher  = {ACM},
  pdf        = {Snapshot:https\://dl.acm.org/citation.cfm?id=2805840:;Fulltext:https\://www.researchgate.net/profile/Mi_Zhang2/publication/310769788_MyBehavior_automatic_personalized_health_feedback_from_user_behaviors_and_preferences_using_smartphones/links/597d5b06aca272d56813210d/MyBehavior-automatic-personalized-health-feedback-from-user-behaviors-and-preferences-using-smartphones.pdf:application/pdf},
  shorttitle = {{MyBehavior}},
}

@InProceedings{Kohavi2007,
  author     = {Kohavi, Ron and Henne, Randal M. and Sommerfield, Dan},
  title      = {Practical Guide to Controlled Experiments on the Web},
  booktitle  = {Proceedings of the 13th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
  year       = {2007},
  pages      = {959--967},
  publisher  = {ACM},
  pdf        = {Snapshot:https\://dl.acm.org/citation.cfm?id=1281295:;Fulltext:http\://courses.cs.washington.edu/courses/cse454/15au/papers/p959-kohavi.pdf:application/pdf},
  shorttitle = {Practical guide to controlled experiments on the web},
}

@Misc{BibEntry2018Aug2,
  author = {Google},
  title  = {Google Analytics},
  month  = aug,
  year   = {2018},
  note   = {[Online; accessed 26. Aug. 2018]},
  url    = {https://marketingplatform.google.com/about/analytics},
}

@Misc{BibEntry2018Aug3,
  author = {Optimizely},
  title  = {Optimizely},
  month  = aug,
  year   = {2018},
  note   = {[Online; accessed 26. Aug. 2018]},
  url    = {https://www.optimizely.com},
}

@Misc{BibEntry2018Aug1,
  author  = {Mixpanel},
  title   = {Mixpanel},
  month   = aug,
  year    = {2018},
  note    = {[Online; accessed 26. Aug. 2018]},
  journal = {Mixpanel},
  url     = {https://mixpanel.com},
}

@Article{Agarwal2016,
  author  = {Agarwal, Alekh and Bird, Sarah and Cozowicz, Markus and Hoang, Luong and Langford, John and Lee, Stephen and Li, Jiaji and Melamed, Dan and Oshri, Gal and Ribas, Oswaldo},
  title   = {Making Contextual Decisions with Low Technical Debt},
  journal = {arXiv preprint arXiv:1606.03966},
  year    = {2016},
  pdf     = {Snapshot:https\://arxiv.org/abs/1606.03966:;Fulltext:https\://arxiv.org/pdf/1606.03966:application/pdf},
}

@InProceedings{Hido2013,
  author     = {Hido, Shohei and Tokui, Seiya and Oda, Satoshi},
  title      = {Jubatus: An Open Source Platform for Distributed Online Machine Learning},
  booktitle  = {{NIPS} 2013 {Workshop} on {Big} {Learning}, {Lake} {Tahoe}},
  year       = {2013},
  pdf        = {Snapshot:https\://pdfs.semanticscholar.org/e081/165c4abd8fb773658db7e411384d6d02fb13.pdf:;Fulltext:https\://pdfs.semanticscholar.org/e081/165c4abd8fb773658db7e411384d6d02fb13.pdf:application/pdf},
  shorttitle = {Jubatus},
}

@InProceedings{Abadi2016,
  author     = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael},
  title      = {Tensorflow: A System for Large-scale Machine Learning.},
  booktitle  = {{OSDI}},
  year       = {2016},
  volume     = {16},
  pages      = {265--283},
  pdf        = {Snapshot:https\://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf:;Fulltext:https\://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf:application/pdf},
  shorttitle = {Tensorflow},
}

@Misc{2018,
  author     = {Yelp},
  title      = {{MOE}: {A} Global, Black Box Optimization Engine for Real World Metric Optimization},
  month      = aug,
  year       = {2018},
  note       = {original-date: 2014-02-24T21:55:56Z},
  shorttitle = {{MOE}},
  url        = {https://github.com/Yelp/MOE},
  urldate    = {2018-08-26TZ},
}

@Misc{BibEntry2018AugA,
  author  = {ABTasty},
  title   = {Ab Tasty},
  month   = aug,
  year    = {2018},
  note    = {[Online; accessed 27. Aug. 2018]},
  journal = {AB Tasty},
  url     = {https://www.abtasty.com},
}

@Misc{BibEntry2018AugB,
  author = {Adobe},
  title  = {Adobe Target},
  month  = aug,
  year   = {2018},
  note   = {[Online; accessed 27. Aug. 2018]},
  url    = {https://www.adobe.com/marketing/target.html},
}

@Article{Riquelme2018,
  author     = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
  title      = {Deep {Bayesian} {Bandits} {Showdown}: {An} {Empirical} {Comparison} of {Bayesian} {Deep} {Networks} for {Thompson} {Sampling}},
  journal    = {arXiv preprint arXiv:1802.09127},
  year       = {2018},
  pdf        = {Snapshot:https\://arxiv.org/abs/1802.09127:;Fulltext:https\://arxiv.org/pdf/1802.09127:application/pdf},
  shorttitle = {Deep {Bayesian} {Bandits} {Showdown}},
}

@Article{Briggs1997,
  author     = {Briggs, Rex and Hollis, Nigel},
  title      = {Advertising on the Web: Is There Response before Click-through?},
  journal    = {Journal of Advertising research},
  year       = {1997},
  volume     = {37},
  number     = {2},
  pages      = {33--46},
  pdf        = {Snapshot:http\://go.galegroup.com/ps/i.do?id=GALE%7CA20965438&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=00218499&p=AONE&sw=w:},
  publisher  = {World Advertising Research Center Ltd.},
  shorttitle = {Advertising on the web},
}

@Article{Bottou2013,
  author     = {Bottou, Léon and Peters, Jonas and Quiñonero-Candela, Joaquin and Charles, Denis X. and Chickering, D. Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  title      = {Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising},
  journal    = {The Journal of Machine Learning Research},
  year       = {2013},
  volume     = {14},
  number     = {1},
  pages      = {3207--3260},
  pdf        = {Snapshot:http\://www.jmlr.org/papers/volume14/bottou13a/bottou13a.pdf:;Fulltext:http\://www.jmlr.org/papers/volume14/bottou13a/bottou13a.pdf:application/pdf},
  publisher  = {JMLR. org},
  shorttitle = {Counterfactual reasoning and learning systems},
}

@InProceedings{steenwinckel2018self,
  author    = {Steenwinckel, Bram and De Backere, Femke and Nelis, Jelle and Ongenae, Femke and De Turck, Filip},
  title     = {Self-learning Algorithms for the Personalised Interaction with People with Dementia},
  booktitle = {Workshops of the Thirty-Second AAAI Conference on Artificial Intelligence},
  year      = {2018},
  pages     = {153--158},
}

@Article{Jordan2015,
  author     = {Jordan, Michael I. and Mitchell, Tom M.},
  title      = {Machine Learning: Trends, Perspectives, and Prospects},
  journal    = {Science},
  year       = {2015},
  volume     = {349},
  number     = {6245},
  pages      = {255--260},
  pdf        = {Snapshot:http\://science.sciencemag.org/content/349/6245/255.short:;Fulltext:http\://www.cs.cmu.edu/~tom/pubs/Science-ML-2015.pdf:application/pdf},
  publisher  = {American Association for the Advancement of Science},
  shorttitle = {Machine learning},
}

@InProceedings{Chu2011,
  author    = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  title     = {Contextual Bandits with Linear Payoff Functions},
  booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
  year      = {2011},
  pages     = {208--214},
  pdf       = {Snapshot:http\://www.jmlr.org/proceedings/papers/v15/chu11a/chu11a.pdf:;Fulltext:http\://www.jmlr.org/proceedings/papers/v15/chu11a/chu11a.pdf:application/pdf},
}

@Article{Slivkins2014,
  author    = {Slivkins, Aleksandrs},
  title     = {Contextual Bandits with Similarity Information},
  journal   = {The Journal of Machine Learning Research},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {2533--2568},
  pdf       = {Snapshot:http\://www.jmlr.org/papers/volume15/slivkins14a/slivkins14a.pdf:;Fulltext:http\://www.jmlr.org/papers/volume15/slivkins14a/slivkins14a.pdf:application/pdf},
  publisher = {JMLR. org},
}

@Article{May2012,
  author  = {May, Benedict C. and Korda, Nathan and Lee, Anthony and Leslie, David S.},
  title   = {Optimistic {Bayesian} Sampling in Contextual-bandit Problems},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {Jun},
  pages   = {2069--2106},
  pdf     = {Snapshot:http\://www.jmlr.org/papers/v13/may12a.html:;Fulltext:http\://www.jmlr.org/papers/volume13/may12a/may12a.pdf:application/pdf},
}

@Misc{2018a,
  author   = {Gelman, Andrew},
  title    = {Don't Call It a Bandit},
  month    = aug,
  year     = {2018},
  abstract = {Here’s why I don’t like the term “multi-armed bandit” to describe the exploration-exploitation tradeoff of inference and decision analysis. First, and less importantly, each slot machine (or “bandit”) only has one arm. Hence it’s many one-armed bandits, not one multi-armed bandit. Second, the basic strategy in these problems is to play on lots of machines …},
  journal  = {Statistical Modeling, Causal Inference, and Social Science},
  language = {en-US},
  pdf      = {Snapshot:https\://andrewgelman.com/2018/08/04/dont-call-bandit/:text/html},
  url      = {https://andrewgelman.com/2018/08/04/dont-call-bandit/},
  urldate  = {CURRENT\_TIMESTAMP},
}

@Article{Robbins1952,
  author    = {Robbins, Herbert},
  title     = {Some Aspects of the Sequential Design of Experiments},
  journal   = {Bulletin of the American Mathematical Society},
  year      = {1952},
  volume    = {58},
  number    = {5},
  pages     = {527--535},
  month     = sep,
  issn      = {0002-9904, 1936-881X},
  abstract  = {Project Euclid - mathematics and statistics online},
  language  = {EN},
  mrnumber  = {MR0050246},
  pdf       = {Full Text PDF:https\://projecteuclid.org/download/pdf_1/euclid.bams/1183517370:application/pdf;Snapshot:https\://projecteuclid.org/euclid.bams/1183517370:text/html},
  publisher = {American Mathematical Society},
  url       = {https://projecteuclid.org/euclid.bams/1183517370},
  urldate   = {CURRENT\_TIMESTAMP},
  zmnumber  = {0049.37009},
}

@Article{Katehakis1986,
  author    = {Katehakis, Michael N. and Derman, Cyrus},
  title     = {Computing Optimal Sequential Allocation Rules in Clinical Trials},
  journal   = {Lecture notes-monograph series},
  year      = {1986},
  pages     = {29--39},
  pdf       = {Snapshot:https\://www.jstor.org/stable/4355518:;Fulltext:http\://www.dtic.mil/get-tr-doc/pdf?AD=ADA170801:application/pdf},
  publisher = {JSTOR},
}

@Article{Swaminathan2015,
  author    = {Swaminathan, Adith and Joachims, Thorsten},
  title     = {Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization.},
  journal   = {Journal of Machine Learning Research},
  year      = {2015},
  volume    = {16},
  number    = {1},
  pages     = {1731--1755},
  pdf       = {Snapshot:http\://www.jmlr.org/papers/volume16/swaminathan15a/swaminathan15a.pdf:text/html;Fulltext:http\://www.jmlr.org/papers/volume16/swaminathan15a/swaminathan15a.pdf:application/pdf},
  timestamp = {2018-10-09},
}

@PhdThesis{Nicol2014,
  author    = {Nicol, Olivier},
  title     = {Data-driven Evaluation of Contextual Bandit Algorithms and Applications to Dynamic Recommendation},
  school    = {Université de Lille I},
  year      = {2014},
  type      = {{PhD} {Thesis}},
  pdf       = {Snapshot:https\://tel.archives-ouvertes.fr/tel-01297407/:text/html;Fulltext:https\://tel.archives-ouvertes.fr/tel-01297407/file/phd_nicol.pdf:application/pdf},
  timestamp = {2018-10-09},
}

@Article{Austin2011,
  author    = {Austin, Peter C.},
  title     = {An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies},
  journal   = {Multivariate behavioral research},
  year      = {2011},
  volume    = {46},
  number    = {3},
  pages     = {399--424},
  pdf       = {Snapshot:https\://www.tandfonline.com/doi/abs/10.1080/00273171.2011.568786:text/html;Fulltext:https\://www.tandfonline.com/doi/full/10.1080/00273171.2011.568786:text/html},
  publisher = {Taylor \& Francis},
  timestamp = {2018-10-10},
}

@Book{Imbens2015,
  title     = {Causal {Inference} in {Statistics}, {Social}, and {Biomedical} {Sciences}},
  publisher = {Cambridge University Press},
  year      = {2015},
  author    = {Imbens, Guido W. and Rubin, Donald B.},
  month     = apr,
  isbn      = {9780521885881},
  note      = {Google-Books-ID: Bf1tBwAAQBAJ},
  abstract  = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including, matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  keywords  = {Business \& Economics / Industrial Management, Health \& Fitness / Health Care Issues, Mathematics / Probability \& Statistics / General, Medical / Epidemiology, Medical / Research, Social Science / Research},
  language  = {en},
  pdf       = {Google Books Link:https\://books.google.nl/books?id=Bf1tBwAAQBAJ:text/html},
  timestamp = {2018-10-10},
}

@Book{Pearl2009,
  title     = {Causality},
  publisher = {Cambridge University Press},
  year      = {2009},
  author    = {Pearl, Judea},
  month     = sep,
  isbn      = {9780521895606},
  note      = {Google-Books-ID: f4nuexsNVZIC},
  abstract  = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  keywords  = {Computers / Intelligence (AI) \& Semantics, Mathematics / History \& Philosophy, Philosophy / Movements / Analytic, Science / Philosophy \& Social Aspects, Social Science / Research},
  language  = {en},
  pdf       = {Google Books Link:https\://books.google.nl/books?id=f4nuexsNVZIC:text/html},
  timestamp = {2018-10-10},
}

@Article{Agarwal2016a,
  author        = {Agarwal, Alekh and Bird, Sarah and Cozowicz, Markus and Hoang, Luong and Langford, John and Lee, Stephen and Li, Jiaji and Melamed, Dan and Oshri, Gal and Ribas, Oswaldo and Sen, Siddhartha and Slivkins, Alex},
  title         = {Making Contextual Decisions with Low Technical Debt},
  journal       = {arXiv:1606.03966 [cs]},
  year          = {2016},
  month         = jun,
  note          = {arXiv: 1606.03966},
  __markedentry = {[robin:]},
  abstract      = {Applications and systems are constantly faced with decisions that require picking from a set of actions based on contextual information. Reinforcement-based learning algorithms such as contextual bandits can be very effective in these settings, but applying them in practice is fraught with technical debt, and no general system exists that supports them completely. We address this and create the first general system for contextual learning, called the Decision Service. Existing systems often suffer from technical debt that arises from issues like incorrect data collection and weak debuggability, issues we systematically address through our ML methodology and system abstractions. The Decision Service enables all aspects of contextual bandit learning using four system abstractions which connect together in a loop: explore (the decision space), log, learn, and deploy. Notably, our new explore and log abstractions ensure the system produces correct, unbiased data, which our learner uses for online learning and to enable real-time safeguards, all in a fully reproducible manner. The Decision Service has a simple user interface and works with a variety of applications: we present two live production deployments for content recommendation that achieved click-through improvements of 25-30\%, another with 18\% revenue lift in the landing page, and ongoing applications in tech support and machine failure handling. The service makes real-time decisions and learns continuously and scalably, while significantly lowering technical debt.},
  keywords      = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
  pdf           = {arXiv\:1606.03966 PDF:http\://www.arxiv.org/pdf/1606.03966.pdf:application/pdf;arXiv.org Snapshot:http\://arxiv.org/abs/1606.03966:text/html},
  timestamp     = {2018-10-10},
  url           = {http://arxiv.org/abs/1606.03966},
  urldate       = {2018-10-10TZ},
}

@Article{Blyth1972,
  author        = {Blyth, Colin R.},
  title         = {On Simpson's Paradox and the Sure-thing Principle},
  journal       = {Journal of the American Statistical Association},
  year          = {1972},
  volume        = {67},
  number        = {338},
  pages         = {364--366},
  __markedentry = {[robin:6]},
  pdf           = {Snapshot:https\://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10482387:text/html},
  publisher     = {Taylor \& Francis},
  timestamp     = {2018-10-10},
}

@Comment{jabref-meta: databaseType:bibtex;}
