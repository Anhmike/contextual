<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Getting started: running simulations • contextual</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Getting started: running simulations">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">contextual</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.9.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/contextual.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/posts.html">Posts</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Nth-iteration-labs/contextual">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Getting started: running simulations</h1>
                        <h4 class="author">Robin van Emden</h4>
            
            <h4 class="date">2018-09-27</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/Nth-iteration-labs/contextual/blob/master/vignettes/contextual.Rmd"><code>vignettes/contextual.Rmd</code></a></small>
      <div class="hidden name"><code>contextual.Rmd</code></div>

    </div>

    
    
<p>Though contextual has been developed to ease the development and comparison of custom Bandits and Policies, it can also be used to evaluate existing ones. In the current vignette we therefor demonstrate how to run simulations with some of contextual’s predefined bandit and policy classes.</p>
<p>Since online advertising is one of the areas where bandit policies have found widespread application, we will use this field as the setting for a basic bandit tutorial. Generally, the goal in online advertising is to determine which out of several ads to serve a visitor to a particular web page. Translated to a bandit setting, in online advertising:</p>
<ul>
<li>The context is usually determined by visitor and web page characteristics.</li>
<li>Arms are represented by the pool of available ads.</li>
<li>An action equals a shown add.</li>
<li>Rewards are determined by a visitor clicking (a reward of 1) or not clicking (a reward of 0) on the shown ad.</li>
</ul>
<p>For our current simulation, we limit the number of advertisements we want to evaluate to three, and set ourselves the objective of finding out which policy would offer us the highest total click-through rate over four hundred impressions.</p>
<div id="comparing-context-free-policies" class="section level4">
<h4 class="hasAnchor">
<a href="#comparing-context-free-policies" class="anchor"></a>Comparing context-free policies</h4>
<p>Before we are able to evaluate any policies, we first have to model our three ads—each with a different probability of generating a click—as the arms of a bandit. For our current simulation we choose to model the ads with the weight-based ContextualBernoulliBandit, as this allows us to set weights determining the average reward probability of each arm. As can be observed in the source code below, for the current simulation, we set the weights of the arms to respectively <span class="math inline">\(\theta_1 = 0.8\)</span>, <span class="math inline">\(\theta_2 = 0.4\)</span> and <span class="math inline">\(\theta_3 = 0.2\)</span>.</p>
<p>We also choose two context-free policies to evaluate and compare:</p>
<ul>
<li><p>EpsilonFirstPolicy: explores the three ads uniformly at random for a preset period and from thereon exploits the ad with the best click-through rate (a type of policy also known as an A/B test). For our current scenario, we set the exploration period to one hundred impressions.</p></li>
<li><p>EpsilonGreedyPolicy: explores one of the ads uniformly at random <span class="math inline">\(\epsilon\)</span> of the time and exploits the ad with the best current click-through rate <span class="math inline">\(1 - \epsilon\)</span> of the time. For our current scenario, we set <span class="math inline">\(\epsilon = 0.4\)</span>.</p></li>
</ul>
<p>Next, we assign the bandit and our two policy instances to two agents. Finally, we assign a list holding both agents to a Simulator instance, set the simulator’s horizon to four hundred and the number of repeats to ten thousand, run the simulation, and plot() its results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load and attach the contextual package.</span>
<span class="kw">library</span>(contextual)
<span class="co"># Define for how long the simulation will run.</span>
horizon &lt;-<span class="st"> </span><span class="dv">400</span>
<span class="co"># Define how many times to repeat the simulation.</span>
simulations &lt;-<span class="st"> </span><span class="dv">10000</span>
<span class="co"># Define the probability that each ad will be clicked.</span>
click_probabilities &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.8</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>)
<span class="co"># Initialize a ContextualBernoulliBandit</span>
bandit &lt;-<span class="st"> </span>ContextualBernoulliBandit<span class="op">$</span><span class="kw">new</span>(<span class="dt">weights =</span> click_probabilities)
<span class="co"># Initialize an EpsilonGreedyPolicy with a 40% exploiration rate.</span>
eg_policy &lt;-<span class="st"> </span>EpsilonGreedyPolicy<span class="op">$</span><span class="kw">new</span>(<span class="dt">epsilon =</span> <span class="fl">0.4</span>)
<span class="co"># Initialize an EpsilonFirstPolicy with a 100 step exploration period.</span>
ef_policy &lt;-<span class="st"> </span>EpsilonFirstPolicy<span class="op">$</span><span class="kw">new</span>(<span class="dt">first =</span> <span class="dv">100</span>)
<span class="co"># Initialize two Agents, binding each policy to a bandit.</span>
ef_agent &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(ef_policy, bandit)
eg_agent &lt;-<span class="st"> </span>Agent<span class="op">$</span><span class="kw">new</span>(eg_policy, bandit)
<span class="co"># Assign both agents to a list.</span>
agents &lt;-<span class="st"> </span><span class="kw">list</span>(ef_agent, eg_agent)
<span class="co"># Initialize Simulator with agent list, horizon, and nr of simulations.</span>
simulator &lt;-<span class="st"> </span>Simulator<span class="op">$</span><span class="kw">new</span>(agents, horizon, simulations, <span class="dt">do_parallel =</span> <span class="ot">FALSE</span>)
<span class="co"># Now run the simulator.</span>
history &lt;-<span class="st"> </span>simulator<span class="op">$</span><span class="kw"><a href="../reference/Simulator.html">run</a></span>()
<span class="co"># Finally, plot the average reward per time step t</span>
<span class="kw">plot</span>(history, <span class="dt">type =</span> <span class="st">"average"</span>, <span class="dt">regret =</span> <span class="ot">FALSE</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)
<span class="co"># And the cumulative reward rate, which equals the Click Through Rate)</span>
<span class="kw">plot</span>(history, <span class="dt">type =</span> <span class="st">"cumulative"</span>, <span class="dt">regret =</span> <span class="ot">FALSE</span>, <span class="dt">rate =</span> <span class="ot">TRUE</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<div class="figure">
<img src="contextual-fig-1.png" style="width:98.0%">
</div>
<p>As can be observed in the figure above, within our horizon of <span class="math inline">\(T = 400\)</span>, EpsilonFirstPolicy has accumulated more rewards than EpsilonGreedytPolicy. It is easy to see why: The winning arm is better than the other two by a margin. That means that EpsilonFirstPolicy has no difficulty in finding the optimal arm within its exploration period of one hundred impressions. Up to that point, EpsilonGreedyPolicy had the advantage of a headstart, as it was already able to exploit for <span class="math inline">\(1- \epsilon\)</span> or sixty percent of the time. But from one hundred impressions on, EpsilonFirstPolicy switches from full exploration to full exploitation mode. In contrast to EpsilonGreedyPolicy, it is now able to exploit the arm that proved best during exploration all of the time. As a result, it can catch up and pass the rewards cumulated by EpsilonGreedyPolicy within less than one hundred and fifty impressions.</p>
</div>
<div id="adding-context" class="section level4">
<h4 class="hasAnchor">
<a href="#adding-context" class="anchor"></a>Adding context</h4>
<p>If that is all we know of our visitors, we expect the results to be stationary over time, and these are the only policies available, the choice is clear: in the current scenario, it is best to deploy EpsilonFirstPolicy (Also: if our bandit represents our visitors’ click behavior realistically, if our policies’ parameters are optimal, etcetera). However, if we have contextual information on our visitors—for instance, their age—we might be able to do better. Let us suggest that we expect that some of our ads are more effective for older visitors, and other ads more effective for younger visitors.</p>
<p>To incorporate this expectation in our simulation, we need to change the way our bandit generates its rewards. Fortunately, in the case of our ContextualBernoulliBandit, the introduction of two contextual features only requires the addition of a single row to its weight matrix—as ContextualBernoulliBandit parses each of the <span class="math inline">\(d\)</span> rows of its weight matrix as a binary contextual feature randomly available on average <span class="math inline">\(1/d\)</span> or 50% of the time.</p>
<p>As can be seen in the source code below, in choosing our weights, we have taken care to keep the average rewards per arm over the two features equal to the rewards per arm in the previous simulation. So we do not expect a substantial difference with the previous simulation’s outcome for context-free policies EpsilonFirstPolicy and EpsilonGreedyPolicy.</p>
<p>We therefore also include the contextual LinUCBDisjointPolicy, which, in assuming its reward function is a linear function of the context, is able to incorporate our new contextual information into its decision-making process. Now let us rerun the simulation:</p>
<pre><code>#                                  +-----+----+-----------&gt; ads: k = 3
#                                  |     |    |
click_probs         &lt;- matrix(c(  0.2,  0.3, 0.1,     # --&gt; d1: old   (p=.5)
                                  0.6,  0.1, 0.1   ), # --&gt; d2: young (p=.5)
                                                      #     features: d = 2

                                  nrow = 2, ncol = 3, byrow = TRUE)

# Initialize a ContextualBernoulliBandit with contextual weights
context_bandit      &lt;- ContextualBernoulliBandit$new(weights = click_probs)
# Initialize LinUCBDisjointPolicy
lucb_policy         &lt;- LinUCBDisjointPolicy$new(0.6)
# Initialize three Agents, binding each policy to a bandit.
ef_agent            &lt;- Agent$new(ef_policy, context_bandit)
eg_agent            &lt;- Agent$new(eg_policy, context_bandit)
lucb_agent          &lt;- Agent$new(lucb_policy, context_bandit)
# Assign all agents to a list.
agents              &lt;- list(ef_agent, eg_agent, lucb_agent)
# Initialize Simulator with agent list, horizon, and nr of simulations.
simulator           &lt;- Simulator$new(agents, horizon, simulations, do_parallel = FALSE)
# Now run the simulator.
history             &lt;- simulator$run()
# And plot the cumulative reward rate again.
plot(history, type = "cumulative", regret = FALSE, rate = TRUE)</code></pre>
<div class="figure">
<img src="contextual-fig-2.png" style="width:98.0%">
</div>
<p>As can be observed in the figure above, both context-free bandit’s results do indeed not do better than before. On the other hand, LinUCBDisjointPolicy does very well, as it is able to map its rewards to the available contextual features. All in all, this time around, you would choose LinUCBDisjointPolicy to implement on your website—with the added advantage that you will now be in a position to incorporate other contextual features to optimize results further.</p>
<p>Of course, the simulations in the current section are not very realistic. One way to ameliorate that would be to write a Bandit subclass with a more complex generative model. More on that in the vignette “Creating your own Policies and Bandits”. Another option would be to evaluate policies on offline datasets. For that, see “Evaluating policies with offline datasets”</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Robin van Emden.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script><script>
  docsearch({
    
    
    apiKey: '7d3ef95d72d5a68e705ce87f9919b959',
    indexName: 'nth_iteration_labs_contextual',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
