% Encoding: UTF-8

@Manual{contextual,
  title        = {contextual: Simulating Contextual Multi-Armed Bandit Problems in R},
  author       = {van Emden, R. and Kaptein, M. and Postma, E.},
  organization = {Jheronimus Academy of Data Science},
  month        = jun,
  year         = {2018},
  journaltitle = {In preparation for submission to The Journal of Statistical Software},
}

@Article{Wilson2014,
  author    = {Wilson, Robert C and Geana, Andra and White, John M and Ludvig, Elliot A and Cohen, Jonathan D},
  title     = {Humans use directed and random exploration to solve the explore--exploit dilemma.},
  journal   = {Journal of Experimental Psychology: General},
  year      = {2014},
  volume    = {143},
  number    = {6},
  pages     = {2074},
  publisher = {American Psychological Association},
}

@Article{Whittle1979,
  author  = {Whittle, P},
  title   = {Discussion on:“Bandit processes and dynamic allocation indices”[JR Statist. Soc. B, 41, 148--177, 1979] by JC Gittins},
  journal = {J. Roy. Statist. Soc. Ser. B},
  year    = {1979},
  volume  = {41},
  pages   = {165},
}

@Article{Bubeck2012,
  author    = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and others},
  title     = {Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  year      = {2012},
  volume    = {5},
  number    = {1},
  pages     = {1--122},
  publisher = {Now Publishers, Inc.},
}

@InProceedings{Langford2008,
  author    = {Langford, John and Zhang, Tong},
  title     = {The epoch-greedy algorithm for multi-armed bandits with side information},
  booktitle = {Advances in neural information processing systems},
  year      = {2008},
  pages     = {817--824},
}

@InCollection{Tewari2017,
  author    = {Tewari, Ambuj and Murphy, Susan A},
  title     = {From ads to interventions: Contextual bandits in mobile health},
  booktitle = {Mobile Health},
  publisher = {Springer},
  year      = {2017},
  pages     = {495--517},
}

@Article{Wang2005a,
  author    = {Wang, Chih-Chun and Kulkarni, Sanjeev R and Poor, H Vincent},
  title     = {Arbitrary side observations in bandit problems},
  journal   = {Advances in Applied Mathematics},
  year      = {2005},
  volume    = {34},
  number    = {4},
  pages     = {903--938},
  publisher = {Elsevier},
}

@InProceedings{Lu2010,
  author    = {Lu, Tyler and P{\'a}l, D{\'a}vid and P{\'a}l, Martin},
  title     = {Contextual multi-armed bandits},
  booktitle = {Proceedings of the Thirteenth international conference on Artificial Intelligence and Statistics},
  year      = {2010},
  pages     = {485--492},
}

@Article{Kaelbling1996,
  author  = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  title   = {Reinforcement learning: A survey},
  journal = {Journal of artificial intelligence research},
  year    = {1996},
  volume  = {4},
  pages   = {237--285},
}

@Article{Abe2003,
  author    = {Abe, Naoki and Biermann, Alan W and Long, Philip M},
  title     = {Reinforcement learning with immediate rewards and linear hypotheses},
  journal   = {Algorithmica},
  year      = {2003},
  volume    = {37},
  number    = {4},
  pages     = {263--293},
  publisher = {Springer},
}

@InProceedings{Strehl2006,
  author       = {Strehl, Alexander L and Mesterharm, Chris and Littman, Michael L and Hirsh, Haym},
  title        = {Experience-efficient learning in associative bandit problems},
  booktitle    = {Proceedings of the 23rd international conference on Machine learning},
  year         = {2006},
  pages        = {889--896},
  organization = {ACM},
}

@Article{Sarkar1991,
  author    = {Sarkar, Jyotirmoy},
  title     = {One-armed bandit problems with covariates},
  journal   = {The Annals of Statistics},
  year      = {1991},
  pages     = {1978--2002},
  publisher = {JSTOR},
}

@Article{Lai1985,
  author    = {Lai, Tze Leung and Robbins, Herbert},
  title     = {Asymptotically efficient adaptive allocation rules},
  journal   = {Advances in applied mathematics},
  year      = {1985},
  volume    = {6},
  number    = {1},
  pages     = {4--22},
  publisher = {Academic Press},
}

@Article{Auer2002,
  author    = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  title     = {Finite-time analysis of the multiarmed bandit problem},
  journal   = {Machine learning},
  year      = {2002},
  volume    = {47},
  number    = {2-3},
  pages     = {235--256},
  publisher = {Springer},
}

@InProceedings{Li2010,
  author       = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  title        = {A contextual-bandit approach to personalized news article recommendation},
  booktitle    = {Proceedings of the 19th international conference on World wide web},
  year         = {2010},
  pages        = {661--670},
  organization = {ACM},
}

@InProceedings{Tang2013,
  author       = {Tang, Liang and Rosales, Romer and Singh, Ajit and Agarwal, Deepak},
  title        = {Automatic ad format selection via contextual bandits},
  booktitle    = {Proceedings of the 22nd ACM international conference on Conference on information \& knowledge management},
  year         = {2013},
  pages        = {1587--1594},
  organization = {ACM},
}

@Misc{Langford2007,
  author   = {Langford, John and Li, Lihong and Strehl, Alex},
  title    = {Vowpal \{{W}\}abbit},
  year     = {2007},
  keywords = {cj-bib},
}

@Article{Kaptein2016,
  author     = {Kaptein, M. C. and Kruijswijk, J. M. A.},
  title      = {Streamingbandit: {A} platform for developing adaptive persuasive systems},
  journal    = {JSS},
  year       = {2016},
  file       = {Snapshot:http\://repository.ubn.ru.nl/bitstream/handle/2066/161901/161901.pdf:;Fulltext:http\://repository.ubn.ru.nl/bitstream/handle/2066/161901/161901.pdf:application/pdf},
  publisher  = {Salzburg: Center for Human-Computer Interaction, Department of Computer Science, University of Salzburg},
  shorttitle = {Streamingbandit},
}

@Electronic{striatum,
  author     = {NTUCSIE-CLLab},
  year       = {2018},
  title      = {striatum: {Contextual} bandit in python},
  url        = {https://github.com/ntucllab/striatum},
  copyright  = {BSD-2-Clause},
  shorttitle = {striatum},
  urldate    = {2018-07-31TZ},
}

@Misc{SMPyBandits,
  author       = {Lilian Besson},
  title        = {{SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits (MAB) Algorithms in Python}},
  howpublished = {Online at: \url{github.com/SMPyBandits/SMPyBandits}},
  year         = {2018},
  note         = {Code at https://github.com/SMPyBandits/SMPyBandits/, documentation at https://smpybandits.github.io/},
  url          = {https://github.com/SMPyBandits/SMPyBandits/},
}

@Electronic{RCore,
  author       = {{R Core Team}},
  year         = {2018},
  title        = {R: A Language and Environment for Statistical Computing},
  language     = {English},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  url          = {https://www.R-project.org},
}

@InProceedings{Li2011,
  author    = {Li, Lihong and Chu, Wei and Langford, John and Wang, Xuanhui},
  title     = {Unbiased {Offline} {Evaluation} of {Contextual}-bandit-based {News} {Article} {Recommendation} {Algorithms}},
  booktitle = {Proceedings of the {Fourth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
  year      = {2011},
  series    = {{WSDM} '11},
  pages     = {297--306},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Contextual bandit algorithms have become popular for online recommendation systems such as Digg, Yahoo! Buzz, and news recommendation in general. Offline evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their partial-label nature. Common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. In this paper, we introduce a replay methodology for contextual bandit algorithm evaluation. Different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. More importantly, our method can provide provably unbiased evaluations. Our empirical results on a large-scale news article recommendation dataset collected from Yahoo! Front Page conform well with our theoretical results. Furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.},
  doi       = {10.1145/1935826.1935878},
  file      = {ACM Full Text PDF:http\://dl.acm.org/ft_gateway.cfm?id=1935878&type=pdf:application/pdf},
  isbn      = {9781450304931},
  keywords  = {benchmark dataset, contextual bandit, multi-armed bandit, offline evaluation, recommendation},
  url       = {http://doi.acm.org/10.1145/1935826.1935878},
  urldate   = {2018-08-01TZ},
}

@InProceedings{Mousavi2016,
  author       = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
  title        = {Deep reinforcement learning: an overview},
  booktitle    = {Proceedings of SAI Intelligent Systems Conference},
  year         = {2016},
  pages        = {426--440},
  organization = {Springer},
}

@InProceedings{Seldin2011,
  author    = {Seldin, Yevgeny and Auer, Peter and Shawe-Taylor, John S and Ortner, Ronald and Laviolette, Fran{\c{c}}ois},
  title     = {PAC-Bayesian analysis of contextual bandits},
  booktitle = {Advances in neural information processing systems},
  year      = {2011},
  pages     = {1683--1691},
}

@Article{Li2017,
  author  = {Li, Yuxi},
  title   = {Deep reinforcement learning: An overview},
  journal = {arXiv preprint arXiv:1701.07274},
  year    = {2017},
}

@InProceedings{Agarwal2014,
  author    = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  title     = {Taming the monster: A fast and simple algorithm for contextual bandits},
  booktitle = {International Conference on Machine Learning},
  year      = {2014},
  pages     = {1638--1646},
}

@Article{Sutton1998e,
  author   = {R. S. Sutton and A. G. Barto},
  title    = {Reinforcement Learning: An Introduction},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {1998},
  volume   = {9},
  number   = {5},
  pages    = {1054},
  month    = sep,
  issn     = {1045-9227},
  doi      = {10.1109/TNN.1998.712192},
  keywords = {Books, Neural networks, Dynamic programming, Machine learning, Learning systems, Artificial intelligence, Artificial neural networks, Bibliographies, Neurofeedback, Function approximation},
}

@Article{Agrawal2011,
  author      = {Shipra Agrawal and Navin Goyal},
  title       = {Analysis of Thompson Sampling for the multi-armed bandit problem},
  journal     = {arXiv},
  year        = {2011},
  abstract    = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time $T$ is $O(\frac{\ln T}{\Delta} + \frac{1}{\Delta^3})$. And, for the $N$-armed bandit problem, the expected regret in time $T$ is $O([(\sum_{i=2}^N \frac{1}{\Delta_i^2})^2] \ln T)$. Our bounds are optimal but for the dependence on $\Delta_i$ and the constant factors in big-Oh.},
  date        = {2011-11-08},
  eprint      = {http://arxiv.org/abs/1111.1797v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1111.1797v3:PDF},
  keywords    = {cs.LG, cs.DS, 68W40, 68Q25, F.2.0},
}

@Article{Eckles2014,
  author      = {Dean Eckles and Maurits Kaptein},
  title       = {Thompson sampling with the online bootstrap},
  journal     = {arXiv},
  year        = {2014},
  abstract    = {Thompson sampling provides a solution to bandit problems in which new observations are allocated to arms with the posterior probability that an arm is optimal. While sometimes easy to implement and asymptotically optimal, Thompson sampling can be computationally demanding in large scale bandit problems, and its performance is dependent on the model fit to the observed data. We introduce bootstrap Thompson sampling (BTS), a heuristic method for solving bandit problems which modifies Thompson sampling by replacing the posterior distribution used in Thompson sampling by a bootstrap distribution. We first explain BTS and show that the performance of BTS is competitive to Thompson sampling in the well-studied Bernoulli bandit case. Subsequently, we detail why BTS using the online bootstrap is more scalable than regular Thompson sampling, and we show through simulation that BTS is more robust to a misspecified error distribution. BTS is an appealing modification of Thompson sampling, especially when samples from the posterior are otherwise not available or are costly.},
  date        = {2014-10-15},
  eprint      = {http://arxiv.org/abs/1410.4009v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1410.4009v1:PDF},
  keywords    = {cs.LG, stat.CO, stat.ML, 68W27, 62L05, G.3; I.2.6},
}

@Article{Agrawal2012a,
  author      = {Shipra Agrawal and Navin Goyal},
  title       = {Thompson Sampling for Contextual Bandits with Linear Payoffs},
  journal     = {arXiv},
  year        = {2012},
  abstract    = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of $\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for this problem.},
  date        = {2012-09-15},
  eprint      = {http://arxiv.org/abs/1209.3352v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1209.3352v4:PDF},
  keywords    = {cs.LG, cs.DS, stat.ML, 68W40, 68Q25, F.2.0},
}

@Article{Auer2003,
  author    = {Auer, Peter},
  title     = {Using {Confidence} {Bounds} for {Exploitation}-exploration {Trade}-offs},
  journal   = {J. Mach. Learn. Res.},
  year      = {2003},
  volume    = {3},
  pages     = {397--422},
  month     = mar,
  issn      = {1532-4435},
  abstract  = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
  file      = {ACM Full Text PDF:http\://dl.acm.org/ft_gateway.cfm?id=944941&type=pdf:application/pdf},
  keywords  = {bandit problem, exploitation-exploration, linear value function, online Learning, reinforcement learning},
  publisher = {JMLR.org},
  url       = {http://dl.acm.org/citation.cfm?id=944919.944941},
  urldate   = {2018-08-03TZ},
}

@PhdThesis{Li2016,
  author   = {Li, Shuai},
  title    = {The art of clustering bandits.},
  school   = {Università degli Studi dell'Insubria},
  year     = {2016},
  abstract = {Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithms could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. The purpose of this thesis is to introduce novel and principled algorithmic approaches to the solution of such networked bandit problems. Starting from a global (Laplacian-based) strategy which allocates a bandit algorithm to each network node (user), and allows it to share signals (contexts and payoffs) with the neghboring nodes, our goal is to derive and experimentally test more scalable approaches based on different ways of clustering the graph nodes. More importantly, we shall investigate the case when the graph structure is not given ahead of time, and has to be inferred based on past user behavior. A general difficulty arising in such practical scenarios is that data sequences are typically nonstationary, implying that traditional statistical inference methods should be used cautiously, possibly replacing them with by more robust nonstochastic (e.g., game-theoretic) inference methods.
In this thesis, we will firstly introduce the centralized clustering bandits. Then, we propose the corresponding solution in decentralized scenario. After that, we explain the generic collaborative clustering bandits. Finally, we extend and showcase the state-of-the-art clustering bandits that we developed in the quantification problem.},
  file     = {Full Text PDF:http\://insubriaspace.cineca.it/bitstream/10277/729/1/PhD_Thesis_Lishuai_completa.pdf:application/pdf;Snapshot:http\://insubriaspace.cineca.it/handle/10277/729:text/html},
  language = {eng},
  url      = {http://insubriaspace.cineca.it/handle/10277/729},
  urldate  = {CURRENT\_TIMESTAMP},
}

@InProceedings{Chu2009,
  author        = {Chu, Wei and Park, Seung-Taek and Beaupre, Todd and Motgi, Nitin and Phadke, Amit and Chakraborty, Seinjuti and Zachariah, Joe},
  title         = {A case study of behavior-driven conjoint analysis on {Yahoo}!: front page today module},
  booktitle     = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
  year          = {2009},
  pages         = {1097--1104},
  publisher     = {ACM},
  __markedentry = {[robin:]},
  file          = {Snapshot:https\://dl.acm.org/citation.cfm?id=1557138:;Fulltext:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3657&rep=rep1&type=pdf:application/pdf},
  shorttitle    = {A case study of behavior-driven conjoint analysis on {Yahoo}!},
}

@Article{Idreos2012,
  author        = {Idreos, S. and Groffen, F. and Nes, N. and Manegold, S. and Mullender, S. and Kersten, M.},
  title         = {{MonetDB}: {Two} {Decades} of {Research} in {Column}-oriented {Database}},
  year          = {2012},
  __markedentry = {[robin:6]},
  file          = {Snapshot:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1004.2814&rep=rep1&type=pdf:;Fulltext:http\://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1004.2814&rep=rep1&type=pdf:application/pdf},
  publisher     = {Citeseer},
  shorttitle    = {{MonetDB}},
}

@Comment{jabref-meta: databaseType:bibtex;}
